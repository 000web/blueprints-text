{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text)  \n",
                "Jens Albrecht, Sidharth Ramachandran, Christian Winkler\n",
                "\n",
                "# Chapter 4:<div class='tocSkip'/>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# How to Prepare Textual Data For Statistics and Machine Learning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Remark<div class='tocSkip'/>\n",
                "\n",
                "The code in this notebook differs slightly from the printed book. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
                "\n",
                "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
                "\n",
                "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
                "\n",
                "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup<div class='tocSkip'/>\n",
                "\n",
                "Set directory locations. If working on Google Colab: copy files and install required libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, os\n",
                "ON_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if ON_COLAB:\n",
                "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
                "    os.system(f'wget {GIT_ROOT}/ch04/setup.py')\n",
                "\n",
                "%run -i setup.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Python Settings<div class=\"tocSkip\"/>\n",
                "\n",
                "Common imports, defaults for formatting in Matplotlib, Pandas etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%run \"$BASE_DIR/settings.py\"\n",
                "\n",
                "%reload_ext autoreload\n",
                "%autoreload 2\n",
                "%config InlineBackend.figure_format = 'png'\n",
                "\n",
                "from IPython.core.interactiveshell import InteractiveShell\n",
                "InteractiveShell.ast_node_interactivity = \"all\"\n",
                "\n",
                "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
                "pd.set_option('display.html.use_mathjax', False)\n",
                "sys.path.append(BASE_DIR + '/packages')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# How to Prepare Textual Data For Statistics and Machine Learning\n",
                "## What you'll learn and what we build\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# A Data Preprocessing Pipeline\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introducing the Data Set: Reddit Self Posts\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading Data into Pandas\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "posts_file = f\"{BASE_DIR}/data/reddit-selfposts/rspct.tsv.gz\"\n",
                "posts_file = f\"{BASE_DIR}/data/reddit-selfposts/rspct_autos.tsv.gz\" ### for faster loads use this subset\n",
                "posts_df = pd.read_csv(posts_file, sep='\\t')\n",
                "\n",
                "subred_file = f\"{BASE_DIR}/data/reddit-selfposts/subreddit_info.csv.gz\"\n",
                "subred_df = pd.read_csv(subred_file).set_index(['subreddit'])\n",
                "\n",
                "df = posts_df.join(subred_df, on='subreddit')\n",
                "len(df) ###"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Standardizing Attribute Names\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(df.columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "column_mapping = {\n",
                "    'id': 'id',\n",
                "    'subreddit': 'subreddit',\n",
                "    'title': 'title',\n",
                "    'selftext': 'text',\n",
                "    'category_1': 'category',\n",
                "    'category_2': 'subcategory',  \n",
                "    'category_3': None, # no data\n",
                "    'in_data': None, # not needed\n",
                "    'reason_for_exclusion': None # not needed\n",
                "}\n",
                "\n",
                "# define remaining columns\n",
                "columns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n",
                "\n",
                "# select and rename those columns\n",
                "df = df[columns].rename(columns=column_mapping)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = df[df['category'] == 'autos']\n",
                "len(df) ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.options.display.max_colwidth = None ###\n",
                "df.sample(1, random_state=7).T\n",
                "pd.options.display.max_colwidth = 200 ###"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Checking for Missing Values\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.isna().sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Saving and Loading a Data Frame\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.to_pickle(\"reddit_dataframe.pkl\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sqlite3\n",
                "\n",
                "db_path = f\"{BASE_DIR}/data/reddit-selfposts/reddit-selfposts.db\"\n",
                "\n",
                "con = sqlite3.connect(db_path)\n",
                "df.to_sql(\"posts\", con, index=False, if_exists=\"replace\")\n",
                "con.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sqlite3 ###\n",
                "db_path = f\"{BASE_DIR}/data/reddit-selfposts/reddit-selfposts.db\" ###\n",
                "con = sqlite3.connect(db_path)\n",
                "df = pd.read_sql(\"select * from posts\", con)\n",
                "con.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Cleaning Textual Data with Regular Expressions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"\"\"\n",
                "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
                "it got me thinking about the best match ups.\n",
                "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
                "Captain America<lb>\"\"\"\n",
                "\n",
                "text = text.replace('\\n', ' ').strip() ###\n",
                "print(text) ###"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Identifying Dirty Data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
                "\n",
                "def impurity(text, min_len=10):\n",
                "    if text == None or len(text) < min_len:\n",
                "        return 0\n",
                "    else:\n",
                "        # return share of suspicious characters in a text\n",
                "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
                "\n",
                "print(impurity(text))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.options.display.max_colwidth = 100 ###\n",
                "# add new column to data frame\n",
                "df['impurity'] = df['text'].progress_apply(impurity, min_len=10)\n",
                "\n",
                "# get the top 3 records\n",
                "df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)\n",
                "pd.options.display.max_colwidth = 200 ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from blueprints.exploration import count_words\n",
                "count_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Text-Cleaning with Regular Expressions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import html\n",
                "\n",
                "def clean(text):\n",
                "    # convert html escapes like &amp; to characters.\n",
                "    text = html.unescape(text) \n",
                "    # tags like <tab>\n",
                "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
                "    # markdown URLs like [Some text](https://....)\n",
                "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
                "    # text or code in brackets like [0]\n",
                "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
                "    # standalone sequences of specials, matches &# but not #cool\n",
                "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
                "    # standalone sequences of hyphens like --- or ==\n",
                "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
                "    # sequences of white spaces\n",
                "    text = re.sub(r'\\s+', ' ', text)\n",
                "    return text.strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clean_text = clean(text)\n",
                "print(clean_text)\n",
                "print(\"Impurity:\", impurity(clean_text))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['clean_text'] = df['text'].progress_apply(clean)\n",
                "df['impurity']   = df['clean_text'].apply(impurity, min_len=20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[['clean_text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Removing Noise with textacy \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from textacy.preprocessing.resources import RE_URL\n",
                "\n",
                "count_words(df, column='clean_text', preprocess=RE_URL.findall).head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pattern-based Data Masking with Textacy\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from textacy.preprocessing.replace import replace_urls\n",
                "\n",
                "text = \"Check out https://spacy.io/usage/spacy-101\"\n",
                "\n",
                "# using default substitution _URL_\n",
                "print(replace_urls(text))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Unicode Character Normalization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"The caf\u00e9 \u201cSaint-Rapha\u00ebl\u201d is loca-\\nted on C\u00f4te d\u02bcAzur.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import textacy.preprocessing as tprep\n",
                "\n",
                "def normalize(text):\n",
                "    text = tprep.normalize_hyphenated_words(text)\n",
                "    text = tprep.normalize_quotation_marks(text)\n",
                "    text = tprep.normalize_unicode(text)\n",
                "    text = tprep.remove_accents(text)\n",
                "    return text\n",
                "\n",
                "print(normalize(text))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['clean_text'] = df['clean_text'].progress_map(normalize)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['text'] = df['clean_text']\n",
                "df.drop(columns=['clean_text', 'impurity'], inplace=True)\n",
                "\n",
                "db_path = f\"{BASE_DIR}/data/reddit-selfposts/reddit-selfposts.db\" ###\n",
                "con = sqlite3.connect(db_path)\n",
                "df.to_sql(\"posts_cleaned\", con, index=False, if_exists=\"replace\")\n",
                "con.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tokenization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"\"\"\n",
                "2019-08-10 23:32: @pete/@louis - I don't have a well-designed \n",
                "solution for today's problem. The code of module AC68 should be -1. \n",
                "Have to think a bit... #goodnight ;-) \ud83d\ude29\ud83d\ude2c\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization with Regular Expressions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokens = re.findall(r'\\w\\w+', text)\n",
                "print(\"|\".join(tokens))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "RE_TOKEN = re.compile(r\"\"\"\n",
                "               ( [#]?[@\\w'\u2019\\.\\-\\:]*\\w     # words, hash tags and email adresses\n",
                "               | [:;<]\\-?[\\)\\(3]          # coarse pattern for basic text emojis\n",
                "               | [\\U0001F100-\\U0001FFFF]  # coarse code range for unicode emojis\n",
                "               )\n",
                "               \"\"\", re.VERBOSE)\n",
                "\n",
                "def tokenize(text):\n",
                "    return RE_TOKEN.findall(text)\n",
                "\n",
                "tokens = tokenize(text)\n",
                "print(\"|\".join(tokens))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['tokens'] = df['text'].progress_map(tokenize)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization with NLTK\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "\n",
                "nltk.download('punkt') ###\n",
                "tokens = nltk.tokenize.word_tokenize(text)\n",
                "print(\"|\".join(t for t in tokens))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Recommendations for Tokenization\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Linguistic Processing with spaCy\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Instantiating a Pipeline\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import spacy\n",
                "nlp = spacy.load('en_core_web_sm')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nlp.pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Processing Text\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nlp = spacy.load(\"en\")\n",
                "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
                "doc = nlp(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for token in doc:\n",
                "    print(token, end=\"|\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def display_nlp(doc, include_punct=False):\n",
                "    \"\"\"Generate data frame for visualization of spaCy tokens.\"\"\"\n",
                "    rows = []\n",
                "    for i, t in enumerate(doc):\n",
                "        if not t.is_punct or include_punct:\n",
                "            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_, \n",
                "                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
                "                   'pos_': t.pos_, 'dep_': t.dep_, 'ent_type_': t.ent_type_}\n",
                "            rows.append(row)\n",
                "    \n",
                "    df = pd.DataFrame(rows).set_index('token')\n",
                "    df.index.name = None\n",
                "    return df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Customizing Tokenization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) \ud83d\ude0b\ud83d\udc4d\"\n",
                "nlp = spacy.load('en') ###\n",
                "doc = nlp(text)\n",
                "\n",
                "print(*[token for token in doc], sep=\"|\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re ###\n",
                "import spacy ###\n",
                "from spacy.tokenizer import Tokenizer\n",
                "from spacy.util import compile_prefix_regex, \\\n",
                "                       compile_infix_regex, compile_suffix_regex\n",
                "\n",
                "def custom_tokenizer(nlp):\n",
                "    \n",
                "    # use default patterns except the ones matched by re.search\n",
                "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
                "                if pattern not in ['-', '_', '#']]\n",
                "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
                "                if pattern not in ['_']]\n",
                "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
                "                if not re.search(pattern, 'xx-xx')]\n",
                "\n",
                "    return Tokenizer(vocab          = nlp.vocab, \n",
                "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
                "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
                "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
                "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
                "                     token_match    = nlp.Defaults.token_match)\n",
                "\n",
                "nlp = spacy.load('en')\n",
                "nlp.tokenizer = custom_tokenizer(nlp)\n",
                "\n",
                "doc = nlp(text)\n",
                "print(*[token for token in doc], sep=\"|\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Lemmatization\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stop Word Detection\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from spacy.lang.en import STOP_WORDS as stop_words\n",
                "print(len(stop_words))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nlp = spacy.load('en')\n",
                "nlp.vocab['down'].is_stop = False\n",
                "nlp.vocab['Dear'].is_stop = True\n",
                "nlp.vocab['Regards'].is_stop = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part-of-Speech Tagging\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ADD? Dependency Parsing and Named Entity Recognition\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"My friend Ryan Peters doesn't like Florida.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from spacy import displacy\n",
                "\n",
                "nlp = spacy.load('en') ###\n",
                "doc = nlp(text)\n",
                "displacy.render(doc, style='dep', options={'compact': False, 'distance': 100})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Named-Entity Recognition\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from spacy import displacy ###\n",
                "\n",
                "text = \"James O'Neill, chairman of World Cargo Inc, lives in San Francisco.\"\n",
                "doc = nlp(text)\n",
                "\n",
                "displacy.render(doc, style='ent', jupyter=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Blueprints for Feature Extraction\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Words based on Part-of-Speech\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
                "doc = nlp(text)\n",
                "\n",
                "nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
                "print(nouns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import textacy\n",
                "\n",
                "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
                "doc = nlp(text)\n",
                "\n",
                "tokens = textacy.extract.words(doc, \n",
                "            filter_stops = True,           # default True, no stopwords\n",
                "            filter_punct = True,           # default True, no punctuation\n",
                "            filter_nums = True,            # default False, no numbers\n",
                "            include_pos = ['NOUN', 'PROPN'], # default None = include all\n",
                "            exclude_pos = None,            # default None = exclude none\n",
                "            min_freq = 1)                  # minimum frequency of words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_lemmas(doc, **kwargs):\n",
                "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
                "\n",
                "lemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])\n",
                "print(*lemmas, sep='|')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Noun Chunks\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spans = textacy.extract.matches(doc, patterns=[\"POS:ADJ:? POS:NOUN:+\"])\n",
                "print(*spans, sep='|')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(*doc.noun_chunks, sep='|')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_noun_chunks(doc, include_pos=['NOUN'], sep='_'):\n",
                "    chunks = []\n",
                "    for noun_chunk in doc.noun_chunks:\n",
                "        chunk = [token.lemma_ for token in noun_chunk\n",
                "                 if token.pos_ in include_pos]\n",
                "        if len(chunk) >= 2:\n",
                "            chunks.append(sep.join(chunk))\n",
                "    return chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "noun_chunks = extract_noun_chunks(doc, include_pos=['ADJ', 'NOUN', 'PROPN'])\n",
                "print(*noun_chunks, sep='|')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Named Entities\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_entities(doc, include_types=None, sep='_'):\n",
                "\n",
                "    ents = textacy.extract.entities(doc, \n",
                "             include_types=include_types, \n",
                "             exclude_types=None, \n",
                "             drop_determiners=True, \n",
                "             min_freq=1)\n",
                "    \n",
                "    return [re.sub('\\s+', sep, e.lemma_)+'/'+e.label_ for e in ents]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nlp = spacy.load('en') ###\n",
                "text = \"George Washington was the first president of the United States.\"\n",
                "doc = nlp(text)\n",
                "\n",
                "entities = extract_entities(doc, ['PERSON', 'GPE'])\n",
                "print(*entities, sep='|')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Extracting NLP Features on a Large Dataset\n",
                "## One Function to Get It All\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nlp = spacy.load('en') # load model\n",
                "nlp.tokenizer = custom_tokenizer(nlp) # optional\n",
                "\n",
                "def extract_nlp(doc):\n",
                "\n",
                "    # doc = nlp(text)\n",
                "    \n",
                "    lemmas          = extract_lemmas(doc, exclude_pos = ['PART', 'PUNCT', \n",
                "                                           'DET', 'PRON', 'SYM', 'SPACE'],\n",
                "                                          filter_stops = False)\n",
                "    adjs_verbs      = extract_lemmas(doc, include_pos = ['ADJ', 'VERB'])\n",
                "    nouns           = extract_lemmas(doc, include_pos = ['NOUN', 'PROPN'])\n",
                "    noun_chunks     = extract_noun_chunks(doc, ['NOUN'])\n",
                "    adj_noun_chunks = extract_noun_chunks(doc, ['NOUN', 'ADJ'])\n",
                "    entities        = extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n",
                "\n",
                "    return lemmas, adjs_verbs, nouns, noun_chunks, adj_noun_chunks, entities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
                "doc = nlp(text)\n",
                "print(*extract_nlp(doc), sep='\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Multiple Columns in a Data Frame\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sqlite3 ###\n",
                "db_path = f\"{BASE_DIR}/data/reddit-selfposts/reddit-selfposts.db\" ###\n",
                "con = sqlite3.connect(db_path)\n",
                "df = pd.read_sql(\"select * from posts_cleaned\", con)\n",
                "con.close()\n",
                "\n",
                "df['text'] = df['title'] + ': ' + df['text']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define column names\n",
                "nlp_columns = ['lemmas', 'adjs_verbs', 'nouns', 'noun_chunks', \n",
                "               'adj_noun_chunks', 'entities']\n",
                "\n",
                "df[nlp_columns] = df.progress_apply(lambda row: nlp_extract(row['text']), \n",
                "                                    axis='columns', result_type='expand')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "count_words(df, 'noun_chunks').head(10).plot(kind='barh', figsize=(8,3)).invert_yaxis()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Persisting the Result\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sqlite3 ###\n",
                "df[nlp_columns] = df[nlp_columns].applymap(lambda items: ' '.join(items))\n",
                "\n",
                "con = sqlite3.connect(db_path) \n",
                "df.to_sql(\"posts_nlp\", con, index=False, if_exists=\"replace\")\n",
                "con.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### A Note on Execution Time\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# There is More\n",
                "## Language Detection\n",
                "## Spell Checking\n",
                "## Token Normalization\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Closing Remarks and Recommendations\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {
                "height": "calc(100% - 180px)",
                "left": "10px",
                "top": "150px",
                "width": "265.638px"
            },
            "toc_section_display": true,
            "toc_window_display": true
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}